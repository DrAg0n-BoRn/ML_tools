{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dragon ML Toolbox - Classification Tutorial\n",
    "\n",
    "This notebook demonstrates the complete workflow for training, evaluating, and explaining a PyTorch classification model using the `dragon-ml-toolbox`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "First, let's import all the necessary components from PyTorch, sklearn, and your toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from your dragon_ml_toolbox package\n",
    "from ml_tools.ML_trainer import MyTrainer\n",
    "from ml_tools.ML_callbacks import EarlyStopping, ModelCheckpoint\n",
    "from ml_tools.keys import LogKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Device\n",
    "\n",
    "We'll automatically select the best available hardware accelerator (CUDA or MPS) or default to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Data\n",
    "\n",
    "We will generate mock data for a binary classification task and wrap it in PyTorch `TensorDataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500, \n",
    "    n_features=15, \n",
    "    n_informative=8, \n",
    "    n_redundant=2, \n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create feature names for later use in SHAP plots\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert data to PyTorch Tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train) # CrossEntropyLoss expects LongTensor for labels\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model, Criterion, and Optimizer\n",
    "\n",
    "Next, we define our neural network architecture, choose a loss function, and select an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_features, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.layer_3 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer_1(x))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_3(x) # No softmax needed here, CrossEntropyLoss handles it\n",
    "        return x\n",
    "\n",
    "# Instantiate the components\n",
    "model = SimpleClassifier(input_features=X_train.shape[1], num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Callbacks\n",
    "\n",
    "We'll set up `ModelCheckpoint` to save the best performing model based on validation loss and `EarlyStopping` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "MONITOR_METRIC = LogKeys.VAL_LOSS\n",
    "\n",
    "# This callback saves the best model state to a directory\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    save_dir=CHECKPOINT_DIR,\n",
    "    monitor=MONITOR_METRIC,\n",
    "    save_best_only=True, \n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback stops training if the validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR_METRIC,\n",
    "    patience=15, # Wait 15 epochs for improvement\n",
    "    min_delta=0.001, # A change smaller than this is not considered an improvement\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize the Trainer\n",
    "\n",
    "Now we can instantiate `MyTrainer`, bringing all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    kind='classification',\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    callbacks=[model_checkpoint, early_stopping] # Add our custom callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Call the `.fit()` method to start the training process. The trainer will automatically handle the training loop, validation, progress bars, and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(epochs=150, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "After training, we first load the weights of the best model saved by `ModelCheckpoint`. Then, we call `.evaluate()` to generate and save a full performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model saved by the callback\n",
    "checkpoint_path = Path(CHECKPOINT_DIR)\n",
    "best_model_path = model_checkpoint.last_best_filepath\n",
    "\n",
    "if best_model_path and best_model_path.exists():\n",
    "    print(f'Loading best model weights from: {best_model_path}')\n",
    "    trainer.model.load_state_dict(torch.load(best_model_path))\n",
    "else:\n",
    "    print('Warning: No best model found. Evaluating with the last model state.')\n",
    "\n",
    "# Define a directory to save all evaluation artifacts\n",
    "EVAL_DIR = Path('tutorial_results') / 'evaluation_report'\n",
    "\n",
    "# Evaluate the model (will use the internal test_dataset)\n",
    "trainer.evaluate(save_dir=EVAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Explain the Model\n",
    "\n",
    "Finally, we can use the `.explain()` method to generate SHAP plots for model interpretability. This helps us understand which features are most important for the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory to save all explanation artifacts\n",
    "EXPLAIN_DIR = Path('tutorial_results') / 'explanation_report'\n",
    "\n",
    "# Generate and save SHAP summary plots\n",
    "trainer.explain(\n",
    "    explain_dataset=test_dataset, # The data to explain (defaults to test_dataset if None)\n",
    "    n_samples=50, # Use 50 samples for the explanation\n",
    "    feature_names=feature_names, # Provide names for the plot\n",
    "    save_dir=EXPLAIN_DIR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}